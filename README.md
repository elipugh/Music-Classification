# Music-Classification

## Stanford CS229 Project
#### With Derek Huang (derekahuang) and Arianna Serafini (aserafini)

In this project we compare and analyze different methods of music genre classification using 2 second samples from the GTZAN dataset. Our most accurate model, a CNN, achieved 82% test accuracy with only 2 seconds of audio. For comparison: after listening to 30 seconds of audio, most humans are able to correctly classify the genre with 65-80% accuracy, with average in the 75% range.

In order to run the `cnn.py`, `svm.py`, or `knn2.py` code, download [preprocessed data](https://drive.google.com/file/d/12mCgkvbmissLh2Vop0bp_t98G8QCaV1E/view?usp=sharing).

In order to run `lin_reg.py` or `knn.py`, download GTZAN [raw audio](https://drive.google.com/open?id=0B0NEMdSW1_-IZnM5cVRSWnk5cVk).

![poster](https://github.com/elipugh/Music-Classification/blob/master/CS229_Poster.png)
